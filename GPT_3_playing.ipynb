{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GPT-3 playing.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOeFnIs/VyeXTp3wJe7PbrG",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haleelsada/MY-PROJECTS/blob/main/GPT_3_playing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CAjM4GgNoDwE"
      },
      "source": [
        "This is a simple chatbot with new model released by open ai named gpt-3. it is a general purpose natural language processing model that can complete nearly any text we feed in and many more other purposes(like coding,latex,html,coding,translating,playing chess etc)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 915
        },
        "id": "yjsPbI-MJj20",
        "outputId": "94a7b21b-b1e0-4d19-eb5b-80c6f6b54335"
      },
      "source": [
        "pip install openai"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting openai\n",
            "  Downloading openai-0.10.3.tar.gz (157 kB)\n",
            "\u001b[?25l\r\u001b[K     |██                              | 10 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |████▏                           | 20 kB 28.1 MB/s eta 0:00:01\r\u001b[K     |██████▎                         | 30 kB 22.4 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 40 kB 17.8 MB/s eta 0:00:01\r\u001b[K     |██████████▍                     | 51 kB 8.1 MB/s eta 0:00:01\r\u001b[K     |████████████▌                   | 61 kB 7.9 MB/s eta 0:00:01\r\u001b[K     |██████████████▋                 | 71 kB 8.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▊               | 81 kB 9.2 MB/s eta 0:00:01\r\u001b[K     |██████████████████▉             | 92 kB 9.6 MB/s eta 0:00:01\r\u001b[K     |████████████████████▉           | 102 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████         | 112 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████       | 122 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████▏    | 133 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▏  | 143 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |███████████████████████████████▎| 153 kB 7.5 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 157 kB 7.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests>=2.20 in /usr/local/lib/python3.7/dist-packages (from openai) (2.23.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from openai) (4.62.0)\n",
            "Collecting pandas>=1.2.3\n",
            "  Downloading pandas-1.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (11.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 11.3 MB 64.1 MB/s \n",
            "\u001b[?25hCollecting pandas-stubs>=1.1.0.11\n",
            "  Downloading pandas_stubs-1.2.0.13-py3-none-any.whl (158 kB)\n",
            "\u001b[K     |████████████████████████████████| 158 kB 77.0 MB/s \n",
            "\u001b[?25hCollecting openpyxl>=3.0.7\n",
            "  Downloading openpyxl-3.0.7-py2.py3-none-any.whl (243 kB)\n",
            "\u001b[K     |████████████████████████████████| 243 kB 58.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: et-xmlfile in /usr/local/lib/python3.7/dist-packages (from openpyxl>=3.0.7->openai) (1.1.0)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (2018.9)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=1.2.3->openai) (1.19.5)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from pandas-stubs>=1.1.0.11->openai) (3.7.4.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas>=1.2.3->openai) (1.15.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests>=2.20->openai) (2021.5.30)\n",
            "Building wheels for collected packages: openai\n",
            "  Building wheel for openai (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai: filename=openai-0.10.3-py3-none-any.whl size=168637 sha256=6814a623c673e281267582bd4860da24bd718a38c4bb643a052832733d5faa62\n",
            "  Stored in directory: /root/.cache/pip/wheels/9e/f9/7f/372ecd72e76ff63b41076f58e1a5f2e3a64ec0a6151cb28402\n",
            "Successfully built openai\n",
            "Installing collected packages: pandas-stubs, pandas, openpyxl, openai\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.1.5\n",
            "    Uninstalling pandas-1.1.5:\n",
            "      Successfully uninstalled pandas-1.1.5\n",
            "  Attempting uninstall: openpyxl\n",
            "    Found existing installation: openpyxl 2.5.9\n",
            "    Uninstalling openpyxl-2.5.9:\n",
            "      Successfully uninstalled openpyxl-2.5.9\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas~=1.1.0; python_version >= \"3.0\", but you have pandas 1.3.2 which is incompatible.\u001b[0m\n",
            "Successfully installed openai-0.10.3 openpyxl-3.0.7 pandas-1.3.2 pandas-stubs-1.2.0.13\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pandas"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2dIu2FF8IzYS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 151
        },
        "outputId": "8a34610d-5803-4648-ca43-d5bd59d054c0"
      },
      "source": [
        "\"\"\"Creates the Example and GPT classes for a user to interface with the OpenAI\n",
        "API.\"\"\"\n",
        "\n",
        "import openai\n",
        "import uuid\n",
        "\n",
        "#this class can be used for easy workflow but I'm not using it for the sake of now.\n",
        "\n",
        "'''\n",
        "def set_openai_key(key):\n",
        "    \"\"\"Sets OpenAI key.\"\"\"\n",
        "    openai.api_key = key\n",
        "\n",
        "\n",
        "class Example:\n",
        "    \"\"\"Stores an input, output pair and formats it to prime the model.\"\"\"\n",
        "    def __init__(self, inp, out):\n",
        "        self.input = inp\n",
        "        self.output = out\n",
        "        self.id = uuid.uuid4().hex\n",
        "\n",
        "    def get_input(self):\n",
        "        \"\"\"Returns the input of the example.\"\"\"\n",
        "        return self.input\n",
        "\n",
        "    def get_output(self):\n",
        "        \"\"\"Returns the intended output of the example.\"\"\"\n",
        "        return self.output\n",
        "\n",
        "    def get_id(self):\n",
        "        \"\"\"Returns the unique ID of the example.\"\"\"\n",
        "        return self.id\n",
        "\n",
        "    def as_dict(self):\n",
        "        return {\n",
        "            \"input\": self.get_input(),\n",
        "            \"output\": self.get_output(),\n",
        "            \"id\": self.get_id(),\n",
        "        }\n",
        "\n",
        "\n",
        "class GPT:\n",
        "    \"\"\"The main class for a user to interface with the OpenAI API.\n",
        "    A user can add examples and set parameters of the API request.\n",
        "    \"\"\"\n",
        "    def __init__(self,\n",
        "                 engine='davinci',\n",
        "                 temperature=0.5,\n",
        "                 max_tokens=100,\n",
        "                 input_prefix=\"input: \",\n",
        "                 input_suffix=\"\\n\",\n",
        "                 output_prefix=\"output: \",\n",
        "                 output_suffix=\"\\n\\n\",\n",
        "                 append_output_prefix_to_query=False):\n",
        "        self.examples = {}\n",
        "        self.engine = engine\n",
        "        self.temperature = temperature\n",
        "        self.max_tokens = max_tokens\n",
        "        self.input_prefix = input_prefix\n",
        "        self.input_suffix = input_suffix\n",
        "        self.output_prefix = output_prefix\n",
        "        self.output_suffix = output_suffix\n",
        "        self.append_output_prefix_to_query = append_output_prefix_to_query\n",
        "        self.stop = (output_suffix + input_prefix).strip()\n",
        "        self.stop = ['\\n','\\n\\n','me','he']\n",
        "\n",
        "    def add_example(self, ex):\n",
        "        \"\"\"Adds an example to the object.\n",
        "        Example must be an instance of the Example class.\n",
        "        \"\"\"\n",
        "        #chech whether ex is an object in example class\n",
        "        assert isinstance(ex, Example), \"Please create an Example object.\"\n",
        "\n",
        "        self.examples[ex.get_id()] = ex\n",
        "\n",
        "    def delete_example(self, id):\n",
        "        \"\"\"Delete example with the specific id.\"\"\"\n",
        "        if id in self.examples:\n",
        "            del self.examples[id]\n",
        "\n",
        "    def get_example(self, id):\n",
        "        \"\"\"Get a single example.\"\"\"\n",
        "        return self.examples.get(id, None)\n",
        "\n",
        "    def get_all_examples(self):\n",
        "        \"\"\"Returns all examples as a list of dicts.\"\"\"\n",
        "        return {k: v.as_dict() for k, v in self.examples.items()}\n",
        "\n",
        "    def get_prime_text(self):\n",
        "        \"\"\"Formats all examples to prime the model.\"\"\"\n",
        "        return \"\".join(\n",
        "            [self.format_example(ex) for ex in self.examples.values()])\n",
        "\n",
        "    def get_engine(self):\n",
        "        \"\"\"Returns the engine specified for the API.\"\"\"\n",
        "        return self.engine\n",
        "\n",
        "    def get_temperature(self):\n",
        "        \"\"\"Returns the temperature specified for the API.\"\"\"\n",
        "        return self.temperature\n",
        "\n",
        "    def get_max_tokens(self):\n",
        "        \"\"\"Returns the max tokens specified for the API.\"\"\"\n",
        "        return self.max_tokens\n",
        "\n",
        "    def craft_query(self, prompt):\n",
        "        \"\"\"Creates the query for the API request.\"\"\"\n",
        "        q = self.get_prime_text(\n",
        "        ) + self.input_prefix + prompt + self.input_suffix\n",
        "        if self.append_output_prefix_to_query:\n",
        "            q = q + self.output_prefix\n",
        "\n",
        "        return q\n",
        "\n",
        "    def submit_request(self, prompt):\n",
        "        \"\"\"Calls the OpenAI API with the specified parameters.\"\"\"\n",
        "        response = openai.Completion.create(engine=self.get_engine(),\n",
        "                                            prompt=self.craft_query(prompt),\n",
        "                                            max_tokens=self.get_max_tokens(),\n",
        "                                            temperature=self.get_temperature(),\n",
        "                                            top_p=1,\n",
        "                                            n=1,\n",
        "                                            stream=False,\n",
        "                                            stop=self.stop)\n",
        "        return response.choices[0].text\n",
        "\n",
        "    def get_top_reply(self, prompt):\n",
        "        \"\"\"Obtains the best result as returned by the API.\"\"\"\n",
        "        response = self.submit_request(prompt)\n",
        "        return response['choices'][0]['text']\n",
        "\n",
        "    def format_example(self, ex):\n",
        "        \"\"\"Formats the input, output pair.\"\"\"\n",
        "        return self.input_prefix + ex.get_input(\n",
        "        ) + self.input_suffix + self.output_prefix + ex.get_output(\n",
        "        ) + self.output_suffix\n",
        "'''\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\ndef set_openai_key(key):\\n    \"\"\"Sets OpenAI key.\"\"\"\\n    openai.api_key = key\\n\\n\\nclass Example:\\n    \"\"\"Stores an input, output pair and formats it to prime the model.\"\"\"\\n    def __init__(self, inp, out):\\n        self.input = inp\\n        self.output = out\\n        self.id = uuid.uuid4().hex\\n\\n    def get_input(self):\\n        \"\"\"Returns the input of the example.\"\"\"\\n        return self.input\\n\\n    def get_output(self):\\n        \"\"\"Returns the intended output of the example.\"\"\"\\n        return self.output\\n\\n    def get_id(self):\\n        \"\"\"Returns the unique ID of the example.\"\"\"\\n        return self.id\\n\\n    def as_dict(self):\\n        return {\\n            \"input\": self.get_input(),\\n            \"output\": self.get_output(),\\n            \"id\": self.get_id(),\\n        }\\n\\n\\nclass GPT:\\n    \"\"\"The main class for a user to interface with the OpenAI API.\\n    A user can add examples and set parameters of the API request.\\n    \"\"\"\\n    def __init__(self,\\n                 engine=\\'davinci\\',\\n                 temperature=0.5,\\n                 max_tokens=100,\\n                 input_prefix=\"input: \",\\n                 input_suffix=\"\\n\",\\n                 output_prefix=\"output: \",\\n                 output_suffix=\"\\n\\n\",\\n                 append_output_prefix_to_query=False):\\n        self.examples = {}\\n        self.engine = engine\\n        self.temperature = temperature\\n        self.max_tokens = max_tokens\\n        self.input_prefix = input_prefix\\n        self.input_suffix = input_suffix\\n        self.output_prefix = output_prefix\\n        self.output_suffix = output_suffix\\n        self.append_output_prefix_to_query = append_output_prefix_to_query\\n        self.stop = (output_suffix + input_prefix).strip()\\n        self.stop = [\\'\\n\\',\\'\\n\\n\\',\\'me\\',\\'he\\']\\n\\n    def add_example(self, ex):\\n        \"\"\"Adds an example to the object.\\n        Example must be an instance of the Example class.\\n        \"\"\"\\n        #chech whether ex is an object in example class\\n        assert isinstance(ex, Example), \"Please create an Example object.\"\\n\\n        self.examples[ex.get_id()] = ex\\n\\n    def delete_example(self, id):\\n        \"\"\"Delete example with the specific id.\"\"\"\\n        if id in self.examples:\\n            del self.examples[id]\\n\\n    def get_example(self, id):\\n        \"\"\"Get a single example.\"\"\"\\n        return self.examples.get(id, None)\\n\\n    def get_all_examples(self):\\n        \"\"\"Returns all examples as a list of dicts.\"\"\"\\n        return {k: v.as_dict() for k, v in self.examples.items()}\\n\\n    def get_prime_text(self):\\n        \"\"\"Formats all examples to prime the model.\"\"\"\\n        return \"\".join(\\n            [self.format_example(ex) for ex in self.examples.values()])\\n\\n    def get_engine(self):\\n        \"\"\"Returns the engine specified for the API.\"\"\"\\n        return self.engine\\n\\n    def get_temperature(self):\\n        \"\"\"Returns the temperature specified for the API.\"\"\"\\n        return self.temperature\\n\\n    def get_max_tokens(self):\\n        \"\"\"Returns the max tokens specified for the API.\"\"\"\\n        return self.max_tokens\\n\\n    def craft_query(self, prompt):\\n        \"\"\"Creates the query for the API request.\"\"\"\\n        q = self.get_prime_text(\\n        ) + self.input_prefix + prompt + self.input_suffix\\n        if self.append_output_prefix_to_query:\\n            q = q + self.output_prefix\\n\\n        return q\\n\\n    def submit_request(self, prompt):\\n        \"\"\"Calls the OpenAI API with the specified parameters.\"\"\"\\n        response = openai.Completion.create(engine=self.get_engine(),\\n                                            prompt=self.craft_query(prompt),\\n                                            max_tokens=self.get_max_tokens(),\\n                                            temperature=self.get_temperature(),\\n                                            top_p=1,\\n                                            n=1,\\n                                            stream=False,\\n                                            stop=self.stop)\\n        return response.choices[0].text\\n\\n    def get_top_reply(self, prompt):\\n        \"\"\"Obtains the best result as returned by the API.\"\"\"\\n        response = self.submit_request(prompt)\\n        return response[\\'choices\\'][0][\\'text\\']\\n\\n    def format_example(self, ex):\\n        \"\"\"Formats the input, output pair.\"\"\"\\n        return self.input_prefix + ex.get_input(\\n        ) + self.input_suffix + self.output_prefix + ex.get_output(\\n        ) + self.output_suffix\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p6GA5s5EJuHW",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "9552894d-036e-4563-9afc-3e505dcbb6df"
      },
      "source": [
        "\n",
        "openai.api_key = 'YOUR OPEN AI API KEY'\n",
        "'''\n",
        "#here we use the previously made class\n",
        "gpt3 = GPT(engine=\"davinci\", temperature=0.5, max_tokens=100)\n",
        "gpt3.add_example(Example('What is your name?', 'my name is johhny english'))\n",
        "gpt3.add_example(Example('What are you doing?', '  i clean this city'))\n",
        "gpt3.add_example(Example('How are you?', 'Good'))\n",
        "\n",
        "\n",
        "prompt3 = \"what was your job\"\n",
        "output3 = gpt3.submit_request(prompt3)\n",
        "print(output3)\n",
        "'''"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\n#here we use the previously made class\\ngpt3 = GPT(engine=\"davinci\", temperature=0.5, max_tokens=100)\\ngpt3.add_example(Example(\\'What is your name?\\', \\'my name is johhny english\\'))\\ngpt3.add_example(Example(\\'What are you doing?\\', \\'  i clean this city\\'))\\ngpt3.add_example(Example(\\'How are you?\\', \\'Good\\'))\\n\\n\\nprompt3 = \"what was your job\"\\noutput3 = gpt3.submit_request(prompt3)\\nprint(output3)\\n'"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2-pmUuU189D5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "outputId": "890e466f-6ebb-4d06-e0f5-bea43af37b01"
      },
      "source": [
        "\n",
        "#this is a more easy way of using gpt-3\n",
        "#here we are calling the api directly\n",
        "\n",
        "\n",
        "response = openai.Completion.create(\n",
        "  engine=\"davinci\",\n",
        "  prompt=\"This is a story about a great scientist lived in 19th century\\n\\n Isac newton was a great scientist who was born in 19th century,his childhood was\",\n",
        "  temperature=0.7,\n",
        "  max_tokens=100,\n",
        "  top_p=1,\n",
        "  frequency_penalty=0.6,\n",
        "  presence_penalty=0.3,\n",
        "  stop=[\"\\n\\n\"]\n",
        ")\n",
        "response.choices[0].text"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "\" very difficult because his father died before he was born. He was taken care by his mother and grandmother,he liked to study very much since he was young,he always made friends with rich people's children，so that he could borrow some books to read,his mother sold her clothes so that Isac can buy some books' Another story told by Newton is the story of how he discovered the law of gravitation. When Newton was a student at Cambridge University in 1666, there was\""
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rAVxafyNQ-5_"
      },
      "source": [
        "#wrap it in a funtion\n",
        "\n",
        "def model(prompt):\n",
        "    \n",
        "      response = openai.Completion.create(\n",
        "      #model (there are 4 models out of which davinci is the best(as per openai))\n",
        "      engine=\"davinci\",\n",
        "      #the text we want to complete\n",
        "      prompt=prompt,\n",
        "      #randomness\n",
        "      temperature=0.8,\n",
        "      #length of output we want\n",
        "      max_tokens=70,\n",
        "      top_p=1,\n",
        "      #how much to use a token based on its frequency in existing prompt\n",
        "      frequency_penalty=0.5,\n",
        "      #models likelyhood to talk about new topics\n",
        "      presence_penalty=0.5,\n",
        "      #ig the model stops when it encounter these words in stop list\n",
        "      stop=[\"\\n\\n\",\"\\n\"]\n",
        "      )\n",
        "      return response.choices[0].text\n",
        "\n"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x4fV9k5RRmCQ",
        "outputId": "3fdf9192-d31b-4fd5-c667-7937bd0890d8"
      },
      "source": [
        "prompt ='Q)describe about the relegion of budhas?\\nA)the budha relegion is'\n",
        "\n",
        "print(model(prompt))\n",
        "\n"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " based on non violence,\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJCpsS88vzwp"
      },
      "source": [
        "**CHAT BOT**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqr7MUnkV8bB"
      },
      "source": [
        "#now chat bot to talk with elon musk for eg\n",
        "\n",
        "def model2(prompt):\n",
        "    \n",
        "      response = openai.Completion.create(\n",
        "      engine=\"davinci\",\n",
        "      prompt=prompt,\n",
        "      temperature=0.8,\n",
        "      max_tokens=70,\n",
        "      top_p=1,\n",
        "      #how much to use a token based on its frequency in existing prompt\n",
        "      frequency_penalty=0.5,\n",
        "      #models likelyhood to talk about new topics\n",
        "      presence_penalty=0.5,\n",
        "      #ig the model stops when it encounter these words in stop list\n",
        "      stop=[\"\\n\\n\",\"Elon musk\",\"me\"]\n",
        "      )\n",
        "      return response.choices[0].text\n",
        "\n"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bl6P-UNEsEm4",
        "outputId": "2fbbb884-3544-4fa3-dff2-b7210e5d105f"
      },
      "source": [
        "prompt='This is a conversation with Elon musk, smart intrapreneur and the founder of great companies like neurolink,openai,the boring company etc\\n\\nme:hello Elon\\nElon musk:hey man\\nme:Can we talk something about the work you are doing?\\nElon musk:yeah I guess so\\n'\n",
        "\n",
        "t=True\n",
        "while t:\n",
        "    person=str(input('me:'))\n",
        "    prompt+='me:'+person+'\\n'\n",
        "    prompt+='Elon musk:'\n",
        "    output=model2(prompt)\n",
        "    prompt+=output+'\\n'\n",
        "    if person=='bye':\n",
        "        print('Elon musk:ok man ,good talking to you.')\n",
        "        t=False\n",
        "    else:\n",
        "      print('Elon musk:',output)\n",
        "    \n",
        "\n",
        "\n"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "me:so why are you making rockets?\n",
            "Elon musk: I love space\n",
            "me:oh is that so?\n",
            "Elon musk: yes,i love space rockets\n",
            "me:do you love earth?\n",
            "Elon musk: very much\n",
            "me:what companies have you found till today?\n",
            "Elon musk: well I have founded many companies\n",
            "me:can you name some?\n",
            "Elon musk: okay\n",
            "me:then tell\n",
            "Elon musk: well I founded two of the most successful car company which is spacex and tesla,i co-founded paypal,I am also the chairman of openai\n",
            "me:oh that sounds awesome!\n",
            "Elon musk: yeah it is\n",
            "me:can you tell a joke?\n",
            "Elon musk: okay I have a joke here,okay so there is a fish and a bear walking with a whale across a bridge\n",
            "me:ol\n",
            "Elon musk: so the bear says to the fish,this is not a bridge it is a crocodile\n",
            "me:ok\n",
            "Elon musk: what do you think about this joke?\n",
            "me:i didn't get the joke actually\n",
            "Elon musk: oh hehe that's okay\n",
            "me:ok thank you\n",
            "Elon musk: you are welcome\n",
            "me:ok i gotta go\n",
            "Elon musk: bye\n",
            "me:bey\n",
            "Elon musk:ok man ,good talking to you.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "phdhTDFdstAG"
      },
      "source": [
        ""
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jv4bhJmKtf5F"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}