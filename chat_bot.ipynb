{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "chat bot.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMfAea1zXQOf05Zt0Ubu7Ig",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/haleelsada/MY-PROJECTS/blob/main/chat_bot.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2LYEyjxVl8VZ"
      },
      "source": [
        "Chat bot trained from reddit conversational data ,designed with pytorch and transformers,hosted in huggingface, You can find a project made by sentdex with this model in youtube. this model is huge by size so dificult to download to computer, You can find the paper in the cornel website,it is made by few microsoft employees"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GF8c9XAIkMQc",
        "outputId": "2006f9a6-919c-4651-8dde-8d7669f06f67"
      },
      "source": [
        "!pip install torch"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TvgPKRrMkT2z"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lX8ArAzqkf4A"
      },
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import torch\n",
        "\n",
        "\n",
        "def load_tokenizer_and_model(model=\"microsoft/DialoGPT-large\"):\n",
        "  \"\"\"\n",
        "    Load tokenizer and model instance for some specific DialoGPT model.\n",
        "  \"\"\"\n",
        "  # Initialize tokenizer and model\n",
        "  print(\"Loading model...\")\n",
        "  tokenizer = AutoTokenizer.from_pretrained(model)\n",
        "  model = AutoModelForCausalLM.from_pretrained(model)\n",
        "  \n",
        "  # Return tokenizer and model\n",
        "  return tokenizer, model\n",
        "\n",
        "\n",
        "def generate_response(tokenizer, model, chat_round, chat_history_ids):\n",
        "  \"\"\"\n",
        "    Generate a response to some user input.\n",
        "  \"\"\"\n",
        "  # Encode user input and End-of-String (EOS) token\n",
        "  new_input_ids = tokenizer.encode(input(\">> You:\") + tokenizer.eos_token, return_tensors='pt')\n",
        "\n",
        "  # Append tokens to chat history\n",
        "  bot_input_ids = torch.cat([chat_history_ids, new_input_ids], dim=-1) if chat_round > 0 else new_input_ids\n",
        "\n",
        "  # Generate response given maximum chat length history of 1250 tokens\n",
        "  chat_history_ids = model.generate(bot_input_ids, max_length=1250, pad_token_id=tokenizer.eos_token_id)\n",
        "  \n",
        "  # Print response\n",
        "  print(\"DialoGPT: {}\".format(tokenizer.decode(chat_history_ids[:, bot_input_ids.shape[-1]:][0], skip_special_tokens=True)))\n",
        "  \n",
        "  # Return the chat history ids\n",
        "  return chat_history_ids\n",
        "\n",
        "\n",
        "def chat_for_n_rounds(n=5):\n",
        "  \"\"\"\n",
        "  Chat with chatbot for n rounds (n = 5 by default)\n",
        "  \"\"\"\n",
        "  \n",
        "  # Initialize tokenizer and model\n",
        "  tokenizer, model = load_tokenizer_and_model()\n",
        "  \n",
        "  # Initialize history variable\n",
        "  chat_history_ids = None\n",
        "  \n",
        "  # Chat for n rounds\n",
        "  for chat_round in range(n):\n",
        "    chat_history_ids = generate_response(tokenizer, model, chat_round, chat_history_ids)\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "  chat_for_n_rounds(5)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ba7kjkGMknbF"
      },
      "source": [
        "if __name__ == '__main__':\n",
        "#while True: #if you want to go with fresh conversation every time\n",
        "  chat_for_n_rounds(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eh2QS12Qotfj"
      },
      "source": [
        "what i think is \n",
        "\n",
        "\n",
        "*   it is slow \n",
        "*   when it came across a tough question or something it repeats the previous chat again and again\n",
        "\n",
        "*   As the value of n increases it got lost in conversation and start acting crazy\n",
        "*   not self conscious\n",
        "*   If answer good in first question\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWNkCGRRlcnv"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}